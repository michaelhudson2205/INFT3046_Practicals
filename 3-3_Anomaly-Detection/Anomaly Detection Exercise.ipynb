{"cells":[{"cell_type":"markdown","id":"424641d8","metadata":{"id":"424641d8"},"source":["# Anomaly Detection\n","\n","### Introduction to anomaly detection\n","\n","*   Also known as Outlier Analysis. An **outlier(anomaly)** is a data object that deviates significantly from the rest of the objects.    \n","\n","  <img src=\"https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780123814791/files/images/F000125f12-01-9780123814791.jpg\">\n","\n","  Fig 1. The objects in region R are outliers\n","\n","*   Types of outliers\n"," *  **Global outlier** (point anomalies) - an outlier deviates significantly from the rest of the data set\n"," *  **Contextual outlier** - an outlier depends on the context—the date, the location, and possibly some other factors\n"," *  **Collective outlier** - a subset of objects as a whole deviate significantly from the entire data set\n","\n"," <img src=\"https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780123814791/files/images/F000125f12-02-9780123814791.jpg\">\n","\n"," Fig 2. The black objects form a collective outlier.\n","\n","*   Challenges of anomaly detection\n"," *  Modeling normal objects and outliers effectively\n"," *  Application-specific outlier detection\n"," *  Handling noise in outlier detection\n"," *  Understandability"]},{"cell_type":"markdown","source":["*   Model development and evaluation (Goldstein & Uchida, 2016):\n","\n"," *  **Supervised anomaly detection** uses **a fully labeled dataset** for training.\n"," *  **Semi-supervised anomaly detection** uses an **anomaly-free training dataset**. Afterwards, deviations in the test data from that normal model are used to detect anomalies.\n"," *  **Unsupervised anomaly detection algorithms** use only **intrinsic information** of the data in order to detect instances deviating from the majority of the data. ***“Although unsupervised anomaly detection does not utilize any label information in practice, they are needed for evaluation and comparison. When new algorithms are proposed, it is common practice that an available public classification dataset is modified and the method is compared with the most known algorithms （(Goldstein & Uchida, 2016， p17)”***\n","\n","* Review from 290 articles published from 2000 to 2020 (Nassif et al., 2021):\n","  * 29 distinct machine learning models\n","  * 22 datasets used, e.g., video anomaly detection, intrusion detection, hyperspectral imagery, medical application etc.\n","  * Gaussian model appeared 2 times out of 290 articles\n","  * unsupervised anomaly detection has been adopted more than other classification anomaly detection systems"],"metadata":{"id":"Tb0OdXwNhMLx"},"id":"Tb0OdXwNhMLx"},{"cell_type":"markdown","source":["###   Example: Credit Card Fraud Detection\n","\n","\n","*   Data can be downloaded from Kaggle: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n","*   **Context**\n","It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n","\n","*  **Content**\n","The dataset contains transactions made by credit cards in September 2013 by European cardholders.\n","This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n","\n","  It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n","\n","  Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"FnhLhS3xhyIh"},"id":"FnhLhS3xhyIh"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"id":"L1xXvyplhwJp"},"id":"L1xXvyplhwJp","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set the working directory, can be removed/replaced with your working directory\n","!pwd\n","import os\n","os.chdir('your path')\n","!pwd"],"metadata":{"id":"NDnT9tdmduBu"},"id":"NDnT9tdmduBu","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import the modules\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from scipy import optimize\n","from scipy.stats import multivariate_normal\n","from sklearn.manifold import TSNE\n","from sklearn.preprocessing import StandardScaler\n","# Setup the environment\n","import warnings\n","warnings.filterwarnings(action='ignore', category=RuntimeWarning) # This will help keep the notebook clean from unnecessary warnings"],"metadata":{"id":"GYOFy-Wxh3Rg"},"id":"GYOFy-Wxh3Rg","execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","# for unzip the data, uncomment if run the first time\n","import zipfile\n","with zipfile.ZipFile(\"archive.zip\",\"r\") as zip_ref:\n","    zip_ref.extractall(\"data\")\n","'''"],"metadata":{"id":"J_-SI1Z2h6Lh"},"id":"J_-SI1Z2h6Lh","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import data to a dataframe\n","df_full = pd.read_csv(os.getcwd() + \"/creditcard.csv\")"],"metadata":{"id":"1d2nD81jh-Cy"},"id":"1d2nD81jh-Cy","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# return the columns\n","print(df_full.columns.values)"],"metadata":{"id":"_-jZerP-h_uJ"},"id":"_-jZerP-h_uJ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df_full.head())"],"metadata":{"id":"HlXpUbyxfDdO"},"id":"HlXpUbyxfDdO","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Subsampling for demonstration purposes. Running the script on the whole dataset takes a while."],"metadata":{"id":"ltMtP9AQiGRx"},"id":"ltMtP9AQiGRx"},{"cell_type":"code","source":["df_sample = df_full.groupby('Class', group_keys=False).apply(lambda x: x.sample(frac=0.05))"],"metadata":{"id":"eyungKgsiBWi"},"id":"eyungKgsiBWi","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Separate feature set and label set"],"metadata":{"id":"0rEF1V4-iKgp"},"id":"0rEF1V4-iKgp"},{"cell_type":"code","source":["# Separating features from the labels\n","# 'Class' is the target variable indicating fraud or no fraud, so it is excluded from the feature set\n","data_features = df_sample.drop(columns=['Class'])"],"metadata":{"id":"ZXFSmKTuLjPn"},"id":"ZXFSmKTuLjPn","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# labels\n","# The target variable 'Class' is stored separately for model training or evaluation\n","data_label = df_sample['Class']\n","data_label.value_counts()"],"metadata":{"id":"jp4ZgOfHiB1S","collapsed":true},"id":"jp4ZgOfHiB1S","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Normalise the features"],"metadata":{"id":"LontemdMiR7J"},"id":"LontemdMiR7J"},{"cell_type":"code","source":["# function to normalise the features using z-score normalisation\n","def featureNormalise(X):\n","\n","    ### write your codes here\n","\n","    return X_norm"],"metadata":{"id":"3EIGRyHZiPw5"},"id":"3EIGRyHZiPw5","execution_count":null,"outputs":[]},{"cell_type":"code","source":["normalised_features = featureNormalise(data_features)"],"metadata":{"id":"kmy72WGSiUAi"},"id":"kmy72WGSiUAi","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Inspect the feature space using tSNE and visualisation\n","\n","Comparing to PCA, tSNE is capable of perserving the local structure of data, thus more suitable for visualising high-dimensional dzata.\n","\n"],"metadata":{"id":"jxGZ3ff5iYJK"},"id":"jxGZ3ff5iYJK"},{"cell_type":"code","source":["# t-Distributed Stochastic Neighbor Embedding (t-SNE)\n","# use tSNE to visualize high-dimensional data: https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n","\n","# Use t-SNE for dimensionality reduction: it helps visualize high-dimensional data by preserving local relationships,\n","# making it easier to identify clusters and patterns in a visual, two or three-dimensional space.\n","# reduce the dimension to 3 in the embedded space\n","tSNE_embedded = TSNE(n_components=3, learning_rate='auto', init='random', perplexity=10).fit_transform(normalised_features)\n","\n","print('t-SNE done!')"],"metadata":{"id":"Zz7lCa5sipMq"},"id":"Zz7lCa5sipMq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# append the tSNE embedded space to the sample dataframe\n","df_sample['tsne-one'] = tSNE_embedded[:,0]\n","df_sample['tsne-two'] = tSNE_embedded[:,1]\n","df_sample['tsne-three'] = tSNE_embedded[:,2]"],"metadata":{"id":"H2_CBIGCiljS"},"id":"H2_CBIGCiljS","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualise the data after tSNE. Code referenced from: https://scikit-learn.org/stable/auto_examples/manifold/plot_swissroll.html#sphx-glr-auto-examples-manifold-plot-swissroll-py\n","fig = plt.figure(figsize=(8, 6))\n","ax = fig.add_subplot(111, projection='3d')\n","fig.add_axes(ax)\n","ax.scatter(\n","    df_sample['tsne-one'], df_sample['tsne-two'], df_sample['tsne-three'], c=df_sample['Class'], s=50, alpha=0.8\n",")\n","ax.set_title('Data distribution in tSNE embedded 3D space')\n","ax.view_init(azim=-66, elev=12)\n","_ = ax.text2D(0.8, 0.05, s='n_samples='+str(len(df_sample)), transform=ax.transAxes)"],"metadata":{"id":"N6l9ugN9iVsa"},"id":"N6l9ugN9iVsa","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Inspect the feature space using PCA and visualisation"],"metadata":{"id":"VbZDVPD0ic76"},"id":"VbZDVPD0ic76"},{"cell_type":"code","source":["# svd factorisation: https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html,\n","# A(input) = USV(h), s contains the singular values of A, u are the eigenvectors of AAh, Vh are the eigenvectors of AhA\n","def pca(X):\n","    # Calculate the covariance matrix of the transposed input data X\n","    ### write your codes here\n","\n","    # Perform SVD on the covariance matrix\n","    # U: Matrix of eigenvectors, which are the principal components\n","    # S: Vector of singular values, indicating the variance captured by each principal component\n","    # _: The right singular vectors (not used here)\n","    ### write your codes here\n","\n","    return U, S\n","\n","# function to convert original data to principal components\n","def projectData(X, U, K):\n","\n","    ### write your codes here\n","\n","    return Z"],"metadata":{"id":"OZKAAAR5i22a"},"id":"OZKAAAR5i22a","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# fit the model on normalised features\n","U, S = pca(normalised_features)"],"metadata":{"id":"l1oN1iXhi58a"},"id":"l1oN1iXhi58a","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# reduce to 3 principal components\n","n_pca = 3\n","pca_reduced = projectData(normalised_features, U, n_pca)\n","print(\"PCA done!\")"],"metadata":{"id":"5nWcu5kni6a6"},"id":"5nWcu5kni6a6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# append the pca reduced feature space to the sample dataframe\n","df_sample['pca-one'] = pca_reduced[:,0]\n","df_sample['pca-two'] = pca_reduced[:,1]\n","df_sample['pca-three'] = pca_reduced[:,2]"],"metadata":{"id":"hPgECTECi-dS"},"id":"hPgECTECi-dS","execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure(figsize=(8, 6))\n","ax = fig.add_subplot(111, projection='3d')\n","fig.add_axes(ax)\n","ax.scatter(\n","    df_sample['pca-one'], df_sample['pca-two'], df_sample['pca-three'], c=df_sample['Class'], s=50, alpha=0.8\n",")\n","ax.set_title('Data distribution in PCA reduced 3D space')\n","ax.view_init(azim=-66, elev=12)\n","_ = ax.text2D(0.8, 0.05, s='n_samples='+str(len(df_sample)), transform=ax.transAxes)"],"metadata":{"id":"ewEuU25QjAIJ"},"id":"ewEuU25QjAIJ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, axs = plt.subplots(figsize=(8, 8), nrows=2)\n","axs[0].scatter(df_sample['pca-one'], df_sample['pca-two'], c=df_sample['Class'])\n","axs[0].set_title('PCA reduced Data - Component 1 vs Component 2')\n","axs[1].scatter(df_sample['pca-one'], df_sample['pca-three'], c=df_sample['Class'])\n","_ = axs[1].set_title('PCA reduced Data - Component 1 vs Component 3')"],"metadata":{"id":"TXDzBmPzjCNE"},"id":"TXDzBmPzjCNE","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Estimating Gaussian Parameters - mean and variance"],"metadata":{"id":"niGDAWMiibJR"},"id":"niGDAWMiibJR"},{"cell_type":"code","source":["# function to estimate Gaussian parameters: mean, variance\n","def estimateGaussian(X):\n","    \"\"\"\n","    Estimates the parameters of a Gaussian distribution using the data.\n","\n","    Args:\n","    X (ndarray): Input data matrix where each row represents a sample and each column a feature.\n","\n","    Returns:\n","    mu (ndarray): Vector of means for each feature.\n","    sigma2 (ndarray): Vector of variances for each feature.\n","    \"\"\"\n","    ### write your codes here\n","\n","    return mu, sigma2"],"metadata":{"id":"op1BYq3tjF_o"},"id":"op1BYq3tjF_o","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Calculate the Gaussian parameters"],"metadata":{"id":"fvrs2ntBjY0B"},"id":"fvrs2ntBjY0B"},{"cell_type":"code","source":["# Calculate the Gaussian parameters for the normalized features\n","mu, sigma2 = estimateGaussian(normalised_features)\n","print(mu, sigma2)"],"metadata":{"id":"VS8JtAcijZeB"},"id":"VS8JtAcijZeB","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Construct the Gaussian distribution"],"metadata":{"id":"glheX7vPjg0i"},"id":"glheX7vPjg0i"},{"cell_type":"code","source":["# apply scipy.stats.multivariate_normal function to generate a multivariate normal random variable based on the Gassuain parameters\n","# refer to: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html for function details\n","distribution = multivariate_normal(mean=mu, cov=np.diag(sigma2))"],"metadata":{"id":"SPlPL7-ujezh"},"id":"SPlPL7-ujezh","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Fit data to the model and estimate the probability for each data point"],"metadata":{"id":"OuULBwc9jmMy"},"id":"OuULBwc9jmMy"},{"cell_type":"code","source":["# apply the probability density function: pdf(x, mean=None, cov=1, allow_singular=False)\n","probs = distribution.pdf(normalised_features)\n","min(probs)"],"metadata":{"id":"Vn_c7hXfjmwS"},"id":"Vn_c7hXfjmwS","execution_count":null,"outputs":[]},{"cell_type":"code","source":["max(probs)"],"metadata":{"id":"Oo3EizJHjrJJ"},"id":"Oo3EizJHjrJJ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Without labels - Select the best threshold using grid search and visual inspection"],"metadata":{"id":"hjfff6xkjuPL"},"id":"hjfff6xkjuPL"},{"cell_type":"code","source":["# function to walk through a range of epsilon values and visualise the outliers\n","def selectThresholdGS(p):\n","  # define a value list to perform the grid search\n","  epsilons = [np.min(p), np.max(p)*1e-06, np.max(p)*1e-05, np.max(p)*1e-04, np.max(p)*1e-03, np.max(p)*1e-02, np.max(p)*1e-01, np.max(p)]\n","  for epsilon in epsilons:\n","    # get the normalies and abnormalies from fitted model\n","    predictions = (p < epsilon).astype(int)\n","    # plot the 3D graph to fit the outliers\n","    fig = plt.figure(figsize=(8, 6))\n","    ax = fig.add_subplot(111, projection='3d')\n","    fig.add_axes(ax)\n","    ax.scatter(\n","        df_sample['tsne-one'], df_sample['tsne-two'], df_sample['tsne-three'], c=predictions, s=50, alpha=0.8)\n","    ax.set_title('Data distribution in tSNE embedded 3D space after fitting')\n","    ax.view_init(azim=-66, elev=12)\n","    _ = ax.text2D(0.8, 0.05, s='epsilon= '+str(epsilon)+' n_samples='+str(len(df_sample)), transform=ax.transAxes)"],"metadata":{"id":"B85fI8WijvBa"},"id":"B85fI8WijvBa","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Apply the threshold selection procedure"],"metadata":{"id":"Asn8LFoZjyzh"},"id":"Asn8LFoZjyzh"},{"cell_type":"code","source":["selectThresholdGS(probs)"],"metadata":{"id":"n2YXtyvtjwpK"},"id":"n2YXtyvtjwpK","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Anomaly Detection"],"metadata":{"id":"ypGPZqEZjGa5"},"id":"ypGPZqEZjGa5"},{"cell_type":"code","execution_count":null,"id":"808d3745","metadata":{"id":"808d3745"},"outputs":[],"source":["# import the modules\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from scipy import optimize\n","from scipy.io import loadmat\n","from scipy.stats import multivariate_normal"]},{"cell_type":"code","execution_count":null,"id":"119da089","metadata":{"id":"119da089"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(action='ignore', category=RuntimeWarning)"]},{"cell_type":"markdown","id":"7f2748d3","metadata":{"id":"7f2748d3"},"source":["#### Load the dataset `ex8data1.mat`"]},{"cell_type":"code","execution_count":null,"id":"b2ff1d95","metadata":{"id":"b2ff1d95"},"outputs":[],"source":["data = loadmat(\"ex8data1.mat\")\n","X = data[\"X\"]\n","print(X.shape)\n","Xval, yval = data[\"Xval\"], data[\"yval\"].ravel()\n","print(Xval.shape, yval.shape)"]},{"cell_type":"code","source":["# original dataset, splitted into X and Xval\n","data"],"metadata":{"id":"uF-7m9s1PHQC","collapsed":true},"id":"uF-7m9s1PHQC","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# X for estimating the parameters - mean variance\n","X"],"metadata":{"id":"qpch6qPkIC2O","collapsed":true},"id":"qpch6qPkIC2O","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Xval for testing the parameters and evaluating the performance\n","Xval"],"metadata":{"id":"Nb6AIny4IF2b","collapsed":true},"id":"Nb6AIny4IF2b","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"0b3f6ad2","metadata":{"id":"0b3f6ad2"},"source":["#### Visualise the dataset with the scatter plot"]},{"cell_type":"code","execution_count":null,"id":"3af89077","metadata":{"id":"3af89077"},"outputs":[],"source":["# use scatter plot to visualise the data distribution for X\n","plt.figure()\n","plt.scatter(X[:, 0], X[:, 1])\n","plt.xlabel(\"Latency (ms)\")\n","plt.ylabel(\"Throughput (mb/s)\")\n","plt.show()"]},{"cell_type":"code","source":["# use scatter plot to visualise the data distribution for Xval\n","plt.figure()\n","plt.scatter(Xval[:, 0], Xval[:, 1])\n","plt.xlabel(\"Latency (ms)\")\n","plt.ylabel(\"Throughput (mb/s)\")\n","plt.show()"],"metadata":{"id":"FrZr4HBtPRcc"},"id":"FrZr4HBtPRcc","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"4a98b738","metadata":{"id":"4a98b738"},"source":["#### Estimating Gaussian Parameters - mean and variance"]},{"cell_type":"code","execution_count":null,"id":"b1407260","metadata":{"id":"b1407260"},"outputs":[],"source":["# function to estimate Gaussian parameters: mean, variance\n","def estimateGaussian(X):\n","\n","    ### write your codes here\n","\n","    return mu, sigma2"]},{"cell_type":"markdown","id":"a05aef8f","metadata":{"id":"a05aef8f"},"source":["#### Calculate the Gaussian parameters"]},{"cell_type":"code","execution_count":null,"id":"bec7df3b","metadata":{"id":"bec7df3b"},"outputs":[],"source":["mu, sigma2 = estimateGaussian(X)\n","print(mu, sigma2)"]},{"cell_type":"markdown","id":"1dcde6e1","metadata":{"id":"1dcde6e1"},"source":["#### Visualise the dataset and the Gaussian anomaly detector"]},{"cell_type":"code","execution_count":null,"id":"c43a20fa","metadata":{"id":"c43a20fa"},"outputs":[],"source":["# function to visualise fitting the Gaussian anomaly detector on the data\n","def visualiseFit(distribution, X):\n","    # Defining Grid Limits\n","    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","    # Creating a Meshgrid\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n","                         np.arange(y_min, y_max, 0.1))\n","    X_plot = np.c_[xx.ravel(), yy.ravel()]\n","\n","    # apply the probability density function to return the probabilities of objects\n","    y_plot = distribution.pdf(X_plot).reshape(xx.shape)\n","\n","    plt.figure()\n","    plt.scatter(X[:, 0], X[:, 1])\n","    plt.contour(xx, yy, y_plot, levels=[1e-20, 1e-17, 1e-14, 1e-11, 1e-8, 1e-5, 1e-2])\n","    plt.show()"]},{"cell_type":"markdown","id":"90bdf3ea","metadata":{"id":"90bdf3ea"},"source":["#### Construct the Gaussian distribution"]},{"cell_type":"code","execution_count":null,"id":"49324eb6","metadata":{"id":"49324eb6"},"outputs":[],"source":["# apply scipy.stats.multivariate_normal function to generate a multivariate normal random variable based on the Gassuain parameters\n","# refer to: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html for function details\n","distribution = ### write your codes here\n","visualiseFit(distribution, X)"]},{"cell_type":"markdown","id":"ac876702","metadata":{"id":"ac876702"},"source":["#### Select the best threshold returning the highest F1 score"]},{"cell_type":"code","execution_count":null,"id":"bcda15e7","metadata":{"id":"bcda15e7"},"outputs":[],"source":["# function to select the best epsilon (threshold) for determining anomalies\n","def selectThreshold(yval, pval):\n","    # Initialize the best F1 score to the lowest possible value\n","    ### write your codes here\n","\n","    step = (np.max(pval) - np.min(pval)) / 1000\n","\n","    # Iterate through potential epsilon values to find the best threshold\n","    for epsilon in np.arange(np.min(pval),\n","                             np.max(pval) + step, step):\n","        predictions = ### write your codes here       # Generate predictions for this threshold\n","        tp = ### write your codes here     # Calculate true positives\n","        fp = ### write your codes here  # Calculate false positives\n","        fn = ### write your codes here  # Calculate false negatives\n","        precision = ### write your codes here\n","        recall = ### write your codes here\n","        F1 = ### write your codes here\n","        # If this F1 score is better, update the best scores and threshold\n","        if F1 > bestF1:\n","            bestF1 = F1\n","            bestEpsilon = epsilon\n","    return bestEpsilon, bestF1"]},{"cell_type":"markdown","id":"fef735ec","metadata":{"id":"fef735ec"},"source":["#### Apply the threshold selection procedure"]},{"cell_type":"code","execution_count":null,"id":"71e09ed1","metadata":{"id":"71e09ed1"},"outputs":[],"source":["# apply the probability density function: pdf(x, mean=None, cov=1, allow_singular=False)\n","pval = distribution.pdf(Xval)\n","bestEpsilon, bestF1 = selectThreshold(yval, pval)  # Get the best threshold and F1 score\n","print(bestEpsilon, bestF1)"]},{"cell_type":"markdown","id":"9cfdaac2","metadata":{"id":"9cfdaac2"},"source":["#### Visualise the dataset and the Gaussian anomaly detector. Outliers need to be indicated."]},{"cell_type":"code","execution_count":null,"id":"d9037d43","metadata":{"id":"d9037d43"},"outputs":[],"source":["def visualiseFitOutliers(distribution, X, outliers):\n","    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n","                         np.arange(y_min, y_max, 0.1))\n","    X_plot = np.c_[xx.ravel(), yy.ravel()]\n","    y_plot = distribution.pdf(X_plot).reshape(xx.shape)\n","\n","    plt.figure()\n","    plt.scatter(X[:, 0], X[:, 1])\n","    plt.scatter(X[outliers, 0], X[outliers, 1], s=100)\n","    plt.contour(xx, yy, y_plot, levels=[1e-20, 1e-17, 1e-14, 1e-11, 1e-8, 1e-5, 1e-2])\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"81cd03cd","metadata":{"id":"81cd03cd"},"outputs":[],"source":["# Given that the two distributions for X and Xval are similar, we can apply the Gaussian model on X using the parameters estimated on Xval\n","p = distribution.pdf(X)\n","outliers = ### write your codes here\n","visualiseFitOutliers(distribution, X, outliers)"]},{"cell_type":"code","source":["pval = distribution.pdf(Xval)\n","outliers = ### write your codes here\n","visualiseFitOutliers(distribution, Xval, outliers)"],"metadata":{"id":"69N8x6THTFPW"},"id":"69N8x6THTFPW","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"87582c84","metadata":{"id":"87582c84"},"source":["### Task - Perform anomaly detection on a high-dimensional dataset `ex8data2.mat`\n","\n","In this exercise, repeat the above process to construct a Gaussian anomaly detector and apply it to the dataset `ex8data2.mat`."]},{"cell_type":"markdown","id":"16fa25eb","metadata":{"id":"16fa25eb"},"source":["#### Load another dataset `ex8data2.mat`"]},{"cell_type":"code","execution_count":null,"id":"e9d0f9f0","metadata":{"id":"e9d0f9f0"},"outputs":[],"source":["data = loadmat(\"ex8data2.mat\")\n","X = data[\"X\"]\n","print(X.shape)\n","Xval, yval = data[\"Xval\"], data[\"yval\"].ravel()\n","print(Xval.shape, yval.shape)"]},{"cell_type":"markdown","id":"ac2f13b6","metadata":{"id":"ac2f13b6"},"source":["#### Construct the Gaussian anomaly detector, select the best threshold and calculate the best F1 score"]},{"cell_type":"code","execution_count":null,"id":"c2cc0615","metadata":{"id":"c2cc0615"},"outputs":[],"source":["# Estimate the Gaussian parameters from the training data\n","mu, sigma2 = ### write your codes here\n","\n","# Create a multivariate normal distribution with the estimated parameters\n","distribution = ### write your codes here\n","\n","# Calculate the probability density function of the training data\n","p = ### write your codes here\n","\n","# Calculate the probability density function of the validation data\n","pval = ### write your codes here\n","\n","# Determine the best threshold epsilon using the F1 score from validation labels\n","bestEpsilon, bestF1 = selectThreshold(yval, pval)\n","\n","print(bestEpsilon, bestF1)  # 1.38e-18 0.615385\n","print(sum(p < bestEpsilon))  # 117"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[{"file_id":"1Rf1JjvxrDzAnPK8mtskhonAJhhD3ZTyl","timestamp":1724992503788}]}},"nbformat":4,"nbformat_minor":5}