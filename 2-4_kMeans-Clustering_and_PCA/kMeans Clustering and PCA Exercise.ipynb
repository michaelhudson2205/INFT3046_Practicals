{"cells":[{"cell_type":"markdown","id":"31c96fbe","metadata":{"id":"31c96fbe"},"source":["# *k*Mmeans Clustering and Principal Component Analysis"]},{"cell_type":"markdown","id":"10feae41","metadata":{"id":"10feae41"},"source":["### *k*Means clustering\n","\n","*   Suppose a data set $D$, contains $n$ objects, distribute the objects in $D$ into $k$ clusters\n","*   Distance measures: **Euclidean**, Manhattan, Minkowski\n","*   Minimise within-cluster sum-of-squared error(SSE), but only local optimal will be achieved\n","*   Characteristics of *k*Means:\n"," *  Needs one input parameters\n"," *  Can be scalable on very large data set\n"," *  Sensitive to outliers\n"," *  Flat geometry, distances between objects\n"," *  Suitable for not too many clusters; when $k$ is unknown, use Elbow method (https://www.scikit-yb.org/en/latest/api/cluster/elbow.html) or Silhouette analysis (https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)\n","* *k*Means clustering algorithm\n"," * Randomly choosing the initial centroid(s)\n"," * Computing the distance between each object and centroid and assign the object to the closest centroid\n"," * Computing the within-cluster sum of squared errors and update the centroid of each cluster\n"," * Iterating the above two steps until the within-cluster sum of squared errors are minimised\n","\n","<img src=\"https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780123814791/files/images/F000101f10-03-9780123814791.jpg\">\n","\n","Fig 2. Clustering of a set of objects using the k-means method; for (b) update cluster centers and reassign objects accordingly (the mean of each cluster is marked by a +)."]},{"cell_type":"code","source":["# import the libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from scipy.io import loadmat\n","from IPython.display import Image"],"metadata":{"id":"3-lwiz8hFy4Y"},"id":"3-lwiz8hFy4Y","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Real world example: Perform image compression using *k*Means"],"metadata":{"id":"yg5VikA6Z5kO"},"id":"yg5VikA6Z5kO"},{"cell_type":"markdown","id":"b26e990a","metadata":{"id":"b26e990a"},"source":["#### Random Initialisation"]},{"cell_type":"markdown","id":"6c17f79e","metadata":{"id":"6c17f79e"},"source":["#### Image Compression with *k*Mmeans"]},{"cell_type":"code","execution_count":null,"id":"674928df","metadata":{"id":"674928df"},"outputs":[],"source":["# visualise the data\n","Image(filename='bird_small.png')"]},{"cell_type":"markdown","id":"61d7e062","metadata":{"id":"61d7e062"},"source":["#### Load the dataset `bird_small.mat`"]},{"cell_type":"code","execution_count":null,"id":"e9d2ca6f","metadata":{"id":"e9d2ca6f"},"outputs":[],"source":["# load the data\n","data = loadmat('bird_small.mat')\n","# The matrix 'A' contains the RGB values of the image\n","A = data[\"A\"]\n","\n","# a 128x128 image with 3 color channels (RGB)\n","print(A.shape)"]},{"cell_type":"markdown","id":"785c8b02","metadata":{"id":"785c8b02"},"source":["#### Reshape the dataset for easier computation"]},{"cell_type":"code","execution_count":null,"id":"770f8913","metadata":{"id":"770f8913"},"outputs":[],"source":["# Normalize and reshape the image data for k-means clustering\n","A = A / 255\n","# Reshape the 3D image array into a 2D array for k-means\n","X = A.reshape(A.shape[0] * A.shape[1], -1)\n","\n","print(X.shape)  # Expected output: (16384, 3), where 16384 = 128*128 (total number of pixels)"]},{"cell_type":"code","execution_count":null,"id":"330223a8","metadata":{"id":"330223a8"},"outputs":[],"source":["# function to generate random initial centroids\n","def kMeansInitCentroids(X, K):\n","    rng = np.random.RandomState(0)\n","    idx = np.arange(X.shape[0])\n","    rng.shuffle(idx)\n","    centroids = X[idx[:K]]\n","    return centroids"]},{"cell_type":"markdown","id":"cf7b54fb","metadata":{"id":"cf7b54fb"},"source":["#### With 16 clusters, run the *k*Means algorithm on the dataset"]},{"cell_type":"code","source":["# function to recompute the new centroids for each cluster after assignments\n","def computeCentroids(X, idx, K):\n","\n","    # Initialize an array to store the new centroids\n","    # The array has K rows (one for each centroid) and the same number of columns as the dataset X\n","    centroids = np.zeros((K, X.shape[1]))\n","\n","    # Loop over each cluster index from 0 to K-1\n","    for i in range(K):\n","        # Select all the data points assigned to the i-th cluster\n","        # The condition idx == i creates a boolean array that is True where the elements of idx match i\n","        # X[idx == i] selects the rows from X where this condition is True\n","        # Calculate the mean of the assigned points along each feature dimension\n","        # np.mean calculates the average across the specified axis (axis=0 computes mean along columns)\n","        # The resulting mean for each feature becomes the new centroid for this cluster\n","        centroids[i] = np.mean(X[idx ==  i], axis=0)\n","    return centroids"],"metadata":{"id":"fk1fW8CeiELq"},"id":"fk1fW8CeiELq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function to compute the distance between each point and centroid\n","def findClosestCentroids(X, centroids):\n","    # Initialize an array to store the index of the closest centroid for each data point\n","    idx = np.zeros(X.shape[0])\n","\n","    # Loop through each data point in the dataset X\n","    for i in range(X.shape[0]):\n","        # Euclidean distance\n","        # Calculate the Euclidean distance between the i-th data point and each centroid\n","        # The operation involves subtracting each centroid from the data point, squaring the result,\n","        # and summing up all the squares.\n","\n","        ### write your codes here\n","\n","        # Find the index of the centroid with the minimum distance to the i-th data point\n","        # The np.argmin function returns the index of the smallest value in the distance array,\n","        # which corresponds to the closest centroid.\n","\n","        ### write your codes here\n","\n","    # Return the array of indices that indicate the closest centroid for each data point\n","    # Each index in idx corresponds to the centroid that is closest to the data point at the same index in X.\n","    return idx"],"metadata":{"id":"MiIEPnDFhzyR"},"id":"MiIEPnDFhzyR","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run kMeans based on maximum number of iterations\n","def runKmeans(X, initial_centroids, max_iter):\n","    # Determine the number of clusters K based on the shape of the initial centroids array\n","    K = initial_centroids.shape[0]\n","\n","    # Set the initial centroids as provided by the user or an automated process\n","    ### write your codes here\n","\n","    # Iterate over the algorithm for the specified number of maximum iterations\n","    # This loop continually updates the centroids based on the data points assigned to each cluster\n","    for _ in range(max_iter):\n","        # Find the closest centroids for each data point in the dataset\n","        # idx is an array where each element is the index of the nearest centroid for each data point\n","        idx = findClosestCentroids(X, centroids)\n","\n","        # Recompute the centroids by calculating the mean of all points assigned to each cluster\n","        # This step updates the centroids to be the center of all points currently assigned to each cluster\n","\n","        ### write your codes here\n","\n","    # Return the final indices of nearest centroids for each data point and the final centroids\n","    # The idx array shows the cluster membership for each data point in the dataset X\n","    # The centroids array contains the coordinates for each centroid after the last iteration\n","    return idx, centroids"],"metadata":{"id":"vcAhw9arhlkk"},"id":"vcAhw9arhlkk","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"f5133357","metadata":{"id":"f5133357"},"outputs":[],"source":["K = 16\n","max_iters = 10\n","initial_centroids = kMeansInitCentroids(X, K)\n","idx, centroids = runKmeans(X, initial_centroids, max_iters)\n","print(idx[:5])\n","print(centroids[:5])"]},{"cell_type":"code","source":["centroids"],"metadata":{"id":"KL3Jmx84jfNI"},"id":"KL3Jmx84jfNI","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"d61c3ff9","metadata":{"id":"d61c3ff9"},"outputs":[],"source":["# convert the features back to 128 x 128\n","X_recovered = centroids[findClosestCentroids(X, centroids).astype(int)]\n","X_recovered = X_recovered.reshape(A.shape[0], A.shape[1], A.shape[2])\n","print(X_recovered.shape)"]},{"cell_type":"code","source":["X_recovered"],"metadata":{"id":"H_YUIo76jWd7","collapsed":true},"id":"H_YUIo76jWd7","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"eba799c2","metadata":{"id":"eba799c2"},"source":["#### Visualise the dataset"]},{"cell_type":"code","execution_count":null,"id":"efa3eb1f","metadata":{"id":"efa3eb1f"},"outputs":[],"source":["# visualise the compressed image\n","plt.figure()\n","plt.subplot(121)\n","plt.imshow(A)\n","plt.xticks([])\n","plt.yticks([])\n","plt.title(\"Original\")\n","plt.subplot(122)\n","plt.imshow(X_recovered)\n","plt.xticks([])\n","plt.yticks([])\n","plt.title(\"Compressed with 16 colors\")\n","plt.show()"]},{"cell_type":"markdown","source":["### Perform *k*Means clustering on `ex7data2.mat`dataset"],"metadata":{"id":"7KON-qNcGeXy"},"id":"7KON-qNcGeXy"},{"cell_type":"markdown","source":["#### Load `extdata2.mat `"],"metadata":{"id":"myS-MXKnZWZr"},"id":"myS-MXKnZWZr"},{"cell_type":"code","source":["# load the data\n","data = loadmat(\"ex7data2.mat\")\n","X = data[\"X\"]\n","\n","# Output the shape of the data to verify loading\n","print(X.shape)"],"metadata":{"id":"-zhXUGOGZXdE"},"id":"-zhXUGOGZXdE","execution_count":null,"outputs":[]},{"cell_type":"code","source":["X"],"metadata":{"id":"U8HrcAzZojWT","collapsed":true},"id":"U8HrcAzZojWT","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Elbow method - choose K\n","\n"],"metadata":{"id":"mDH0_lq7W37p"},"id":"mDH0_lq7W37p"},{"cell_type":"code","source":["# Implement the Elbow Method to determine the optimal number of clusters\n","import matplotlib.pyplot as plt\n","from sklearn.cluster import KMeans\n","# within-cluster sum of squares (WCSS) - measures the total variance within each cluster.\n","# The smaller the WCSS, the more homogeneous the cluster is.\n","wcss = []\n","\n","for i in range(1, 11):\n","  kmeans = KMeans(n_clusters=i, random_state=42)\n","  kmeans.fit(X)\n","  wcss.append(kmeans.inertia_) # Compute within-cluster sum of squares\n","plt.plot(range(1, 11), wcss)\n","plt.title('The Elbow Method')\n","plt.xlabel('Number of clusters')\n","plt.ylabel('WCSS')\n","plt.show()\n"],"metadata":{"id":"RhICNPkzWv-7"},"id":"RhICNPkzWv-7","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"40ba0728","metadata":{"id":"40ba0728"},"source":["#### Assign initial centroids where the number of clusters is 3 and find the closest centroids"]},{"cell_type":"markdown","id":"08c8220d","metadata":{"id":"08c8220d"},"source":["#### Finding the closest centroid"]},{"cell_type":"code","source":["# initialise 3 centroids to form 3 clusters with random values\n","K = 3\n","\n","# specify the maximum number of iterations to be 10 as kmeans returns local optimal only\n","max_iters = 10\n","\n","# # random centroids\n","# aer = np.random.randn(K, 2)\n","# initial_centroids = np.array(aer)\n","\n","# Use the first three data points as initial centroids instead of predefined points\n","initial_centroids = X[:3]"],"metadata":{"id":"qpIk7xBFlBm9"},"id":"qpIk7xBFlBm9","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"0c0c0bc1","metadata":{"id":"0c0c0bc1"},"outputs":[],"source":["# function to compute the distance between each point and centroid\n","def findClosestCentroids(X, centroids):\n","    # Initialize an array to store the index of the closest centroid for each data point\n","\n","    ### write your codes here\n","\n","    # Loop through each data point in the dataset X\n","    for i in range(X.shape[0]):\n","        # Euclidean distance\n","        # Calculate the Euclidean distance between the i-th data point and each centroid\n","        # The operation involves subtracting each centroid from the data point, squaring the result,\n","        # and summing up all the squares.\n","\n","        ### write your codes here\n","\n","        # Find the index of the centroid with the minimum distance to the i-th data point\n","        # The np.argmin function returns the index of the smallest value in the distance array,\n","        # which corresponds to the closest centroid.\n","\n","        ### write your codes here\n","\n","    # Return the array of indices that indicate the closest centroid for each data point\n","    # Each index in idx corresponds to the centroid that is closest to the data point at the same index in X.\n","    return idx"]},{"cell_type":"code","source":["idx = findClosestCentroids(X, initial_centroids)\n","print(idx[:3])"],"metadata":{"id":"lZY1gWEiyVjN"},"id":"lZY1gWEiyVjN","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"3bad738f","metadata":{"id":"3bad738f"},"source":["#### Run K-means and update centroids"]},{"cell_type":"code","execution_count":null,"id":"d4b51f9b","metadata":{"id":"d4b51f9b"},"outputs":[],"source":["# run kMeans based on maximum number of iterations\n","def runKmeans(X, initial_centroids, max_iter):\n","    # Determine the number of clusters K based on the shape of the initial centroids array\n","\n","    ### write your codes here\n","\n","    # Set the initial centroids as provided by the user or an automated process\n","    ### write your codes here\n","\n","    # Iterate over the algorithm for the specified number of maximum iterations\n","    # This loop continually updates the centroids based on the data points assigned to each cluster\n","    for _ in range(max_iter):\n","        # Find the closest centroids for each data point in the dataset\n","        # idx is an array where each element is the index of the nearest centroid for each data point\n","\n","        ### write your codes here\n","\n","        # Recompute the centroids by calculating the mean of all points assigned to each cluster\n","        # This step updates the centroids to be the center of all points currently assigned to each cluster\n","\n","        ### write your codes here\n","\n","    # Return the final indices of nearest centroids for each data point and the final centroids\n","    # The idx array shows the cluster membership for each data point in the dataset X\n","    # The centroids array contains the coordinates for each centroid after the last iteration\n","    return idx, centroids"]},{"cell_type":"code","execution_count":null,"id":"9c3c327a","metadata":{"id":"9c3c327a"},"outputs":[],"source":["# function to recompute the new centroids for each cluster after assignments\n","def computeCentroids(X, idx, K):\n","\n","    # Initialize an array to store the new centroids\n","    # The array has K rows (one for each centroid) and the same number of columns as the dataset X\n","\n","    ### write your codes here\n","\n","    # Loop over each cluster index from 0 to K-1\n","    for i in range(K):\n","        # Select all the data points assigned to the i-th cluster\n","        # The condition idx == i creates a boolean array that is True where the elements of idx match i\n","        # X[idx == i] selects the rows from X where this condition is True\n","        # Calculate the mean of the assigned points along each feature dimension\n","        # np.mean calculates the average across the specified axis (axis=0 computes mean along columns)\n","        # The resulting mean for each feature becomes the new centroid for this cluster\n","\n","        ### write your codes here\n","\n","    return centroids"]},{"cell_type":"code","execution_count":null,"id":"2de6fe17","metadata":{"id":"2de6fe17"},"outputs":[],"source":["centroids = computeCentroids(X, idx, K)\n","print(centroids)"]},{"cell_type":"code","source":["# run kMeans on the dataset\n","idx, centroids = runKmeans(X, centroids, max_iters)"],"metadata":{"id":"-OsLY8WQVAqV"},"id":"-OsLY8WQVAqV","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"21b4d794","metadata":{"id":"21b4d794"},"source":["#### Visualise the dataset using scatter plot"]},{"cell_type":"code","execution_count":null,"id":"d0ab2b70","metadata":{"id":"d0ab2b70"},"outputs":[],"source":["# plot the clusters and mark the centroids\n","plt.figure()\n","plt.scatter(X[idx == 0, 0], X[idx == 0, 1], color='red')\n","plt.scatter(X[idx == 1, 0], X[idx == 1, 1], color='green')\n","plt.scatter(X[idx == 2, 0], X[idx == 2, 1], color='blue')\n","plt.scatter(centroids[:, 0], centroids[:, 1], color='black', marker=\"+\", s=200)\n","plt.show()"]},{"cell_type":"markdown","id":"6a14f43b","metadata":{"id":"6a14f43b"},"source":["### Principal Component Analysis\n","\n","*   Unsupervised dimensionality reduction technique\n","*   Uses Singular Value Decomposition(SVD) of the data to project it to a lower dimensional space\n","*   Useful in processing high-dimensional data where multi-colinearity exists between the features\n","*   Can be used for denoising and data compression\n","\n","<img src=\"https://miro.medium.com/max/640/1*P8_C9uk3ewpRDtevf9wVxg.png\">\n","\n","Fig 3. Our original data in the xy-plane. (Source: http://setosa.io/ev/principal-component-analysis/)\n","\n","<img src=\"https://miro.medium.com/max/640/1*wsezmnzg-0N_RP3meYNXlQ.png\">\n","\n","Fig 4. Our original data transformed by PCA. (Source: http://setosa.io/ev/principal-component-analysis/)\n"]},{"cell_type":"markdown","source":["### Real-world example: Perform facial image compression using PCA"],"metadata":{"id":"KYA-rC0xewN7"},"id":"KYA-rC0xewN7"},{"cell_type":"markdown","id":"8ff827e0","metadata":{"id":"8ff827e0"},"source":["#### Face Image Dataset"]},{"cell_type":"code","execution_count":null,"id":"9cf4fb98","metadata":{"id":"9cf4fb98"},"outputs":[],"source":["# load the data\n","data = loadmat(\"ex7faces.mat\")\n","X = data[\"X\"]\n","\n","print(X.shape)"]},{"cell_type":"code","execution_count":null,"id":"9cc3408f","metadata":{"id":"9cc3408f"},"outputs":[],"source":["# visualise a sample batch\n","plt.figure(figsize=(10, 10))\n","for i in range(100):\n","    plt.subplot(10, 10, i + 1)\n","    plt.imshow(X[i].reshape((32, 32)).T,\n","               cmap=plt.cm.bone, interpolation='nearest')\n","    plt.xticks([])\n","    plt.yticks([])\n","plt.show()"]},{"cell_type":"markdown","id":"66841bdf","metadata":{"id":"66841bdf"},"source":["#### Normalise the dataset"]},{"cell_type":"code","source":["# function to normalise the features using z-score normalisation\n","def featureNormalise(X):\n","    # Calculate the mean of each feature/column\n","    ### write your codes here\n","\n","    # Calculate the standard deviation of each feature/column, ddof=1 ensures sample standard deviation\n","    ### write your codes here\n","\n","    # Normalize the features by subtracting the mean and dividing by the standard deviation\n","    ### write your codes here\n","\n","\n","    return X_norm, mu, sigma\n","\n","\n","# svd factorisation: https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html,\n","# Singular Value Decomposition (SVD) based PCA implementation\n","# A(input) = USV(h), s contains the singular values of A, u are the eigenvectors of AAh, Vh are the eigenvectors of AhA\n","def pca(X):\n","    # Compute the covariance matrix of the normalized features\n","    # Sigma represents covariance when X is normalized\n","    ### write your codes here\n","\n","    # Perform SVD on the covariance matrix\n","    # U will contain the principal components (eigenvectors of the covariance matrix)\n","    # S will contain the singular values, which are related to the eigenvalues of the covariance matrix\n","    ### write your codes here\n","\n","    return U, S"],"metadata":{"id":"w3V56Ich4NnM"},"id":"w3V56Ich4NnM","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"7b5ff217","metadata":{"id":"7b5ff217"},"outputs":[],"source":["# apply pca on the data\n","X_norm, mu, sigma = featureNormalise(X)\n","U, S = pca(X_norm)"]},{"cell_type":"markdown","id":"80315ce4","metadata":{"id":"80315ce4"},"source":["#### Visualise U matrix"]},{"cell_type":"code","execution_count":null,"id":"eccea2f8","metadata":{"id":"eccea2f8"},"outputs":[],"source":["# visaulise the sample batch after pca on each dimension\n","plt.figure(figsize=(6, 6))\n","for i in range(36):\n","    plt.subplot(6, 6, i + 1)\n","    plt.imshow(U[:, i].reshape((32, 32)).T,\n","               cmap=plt.cm.bone, interpolation='nearest')\n","    plt.xticks([])\n","    plt.yticks([])\n","plt.show()"]},{"cell_type":"markdown","id":"3e94722e","metadata":{"id":"3e94722e"},"source":["#### Reduce the dimension to 100"]},{"cell_type":"code","source":["# Function to project original data onto a lower dimensional space defined by the principal components\n","def projectData(X, U, K):\n","    \"\"\"\n","    Projects the data X onto the top K principal components.\n","\n","    Parameters:\n","    X : array_like\n","        The data set, where each row is an example and each column represents a feature.\n","    U : array_like\n","        The matrix of principal components; each column is a principal component.\n","    K : int\n","        The number of principal components to project onto.\n","\n","    Returns:\n","    Z : array_like\n","        The projected data. This output matrix is of shape (number of examples, K).\n","    \"\"\"\n","    # Use numpy dot product to project the data onto the first K columns of U\n","    # X: original data matrix, U[:, :K]: first K principal components (eigenvectors)\n","    Z = np.dot(X, U[:, :K])\n","    return Z"],"metadata":{"id":"2xSx-iWj8XjE"},"id":"2xSx-iWj8XjE","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"0e25584a","metadata":{"id":"0e25584a"},"outputs":[],"source":["# project data using 100 principal components\n","K = 100\n","Z = projectData(X_norm, U, K)\n","print(Z.shape)"]},{"cell_type":"markdown","id":"bb54d002","metadata":{"id":"bb54d002"},"source":["#### Reconstruct the original dataset"]},{"cell_type":"code","source":["# Function to convert principal components back to the original dimension\n","def recoverData(Z, U, K):\n","    \"\"\"\n","    Reconstructs the original data from its projection onto the top K principal components.\n","\n","    Parameters:\n","    Z : array_like\n","        The projected data, where each row is a data point in the reduced dimensional space.\n","    U : array_like\n","        The matrix of principal components; each column is a principal component.\n","    K : int\n","        The number of principal components that the original data was projected onto.\n","\n","    Returns:\n","    X_rec : array_like\n","        The reconstructed data from the projection. This output has the same number of features as the original dataset.\n","    \"\"\"\n","    # Use numpy dot product to reconstruct the data from its projections\n","    # Z: projected data matrix, U[:, :K].T: transpose of the first K principal components\n","    X_rec = np.dot(Z, U[:, :K].T)\n","    return X_rec\n"],"metadata":{"id":"Zqa8Buxe8fAU"},"id":"Zqa8Buxe8fAU","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"843d0bfa","metadata":{"id":"843d0bfa"},"outputs":[],"source":["X_rec  = recoverData(Z, U, K)\n","print(X_rec.shape)"]},{"cell_type":"markdown","id":"fa212532","metadata":{"id":"fa212532"},"source":["#### Visualise the reconstructed dataset"]},{"cell_type":"code","execution_count":null,"id":"22bd5701","metadata":{"id":"22bd5701"},"outputs":[],"source":["# visualise the sample batch with 100 principal components\n","plt.figure(figsize=(10, 10))\n","for i in range(100):\n","    plt.subplot(10, 10, i + 1)\n","    plt.imshow(X_rec[i].reshape((32, 32)).T,\n","               cmap=plt.cm.bone, interpolation='nearest')\n","    plt.xticks([])\n","    plt.yticks([])\n","plt.show()"]},{"cell_type":"markdown","source":["### Perform PCA on `ex7data1.mat` dataset\n"],"metadata":{"id":"icgNZu-SeBkf"},"id":"icgNZu-SeBkf"},{"cell_type":"markdown","id":"3019d3e5","metadata":{"id":"3019d3e5"},"source":["#### Load the dataset `ex7data1.mat`"]},{"cell_type":"code","execution_count":null,"id":"b5d6eb95","metadata":{"id":"b5d6eb95"},"outputs":[],"source":["data = loadmat(\"ex7data1.mat\")\n","X = data[\"X\"]\n","print(X.shape)"]},{"cell_type":"markdown","id":"84e5263a","metadata":{"id":"84e5263a"},"source":["#### Visualise the dataset using the scatterplot"]},{"cell_type":"code","execution_count":null,"id":"49144ac1","metadata":{"id":"49144ac1"},"outputs":[],"source":["plt.figure()\n","plt.scatter(X[:, 0], X[:, 1])\n","plt.show()"]},{"cell_type":"markdown","id":"63d54f58","metadata":{"id":"63d54f58"},"source":["#### PCA Algorithm"]},{"cell_type":"code","execution_count":null,"id":"cb4c1b88","metadata":{"id":"cb4c1b88"},"outputs":[],"source":["# function to normalise the features using z-score normalisation\n","def featureNormalise(X):\n","    # Calculate the mean of each feature/column\n","    ### write your codes here\n","\n","    # Calculate the standard deviation of each feature/column, ddof=1 ensures sample standard deviation\n","    ### write your codes here\n","\n","    # Normalize the features by subtracting the mean and dividing by the standard deviation\n","    ### write your codes here\n","\n","    return X_norm, mu, sigma\n","\n","\n","# svd factorisation: https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html,\n","# Singular Value Decomposition (SVD) based PCA implementation\n","# A(input) = USV(h), s contains the singular values of A, u are the eigenvectors of AAh, Vh are the eigenvectors of AhA\n","def pca(X):\n","    # Compute the covariance matrix of the normalized features\n","    # Sigma represents covariance when X is normalized\n","    ### write your codes here\n","\n","    # Perform SVD on the covariance matrix\n","    # U will contain the principal components (eigenvectors of the covariance matrix)\n","    # S will contain the singular values, which are related to the eigenvalues of the covariance matrix\n","    ### write your codes here\n","\n","    return U, S"]},{"cell_type":"markdown","id":"91ace9c2","metadata":{"id":"91ace9c2"},"source":["#### Normalise the dataset and apply the PCA"]},{"cell_type":"code","execution_count":null,"id":"cba08db1","metadata":{"id":"cba08db1"},"outputs":[],"source":["X_norm, mu, sigma = featureNormalise(X)\n","U, S = pca(X_norm)"]},{"cell_type":"code","execution_count":null,"id":"a825765f","metadata":{"id":"a825765f"},"outputs":[],"source":["print(U[:, 0])"]},{"cell_type":"markdown","id":"115fdae0","metadata":{"id":"115fdae0"},"source":["#### Visualise the dataset"]},{"cell_type":"code","execution_count":null,"id":"79976923","metadata":{"id":"79976923"},"outputs":[],"source":["# major trends in the data captured by PCA\n","plt.figure(figsize=(4, 4))\n","plt.scatter(X[:, 0], X[:, 1])\n","plt.plot([mu[0], mu[0] + 1.5 * S[0] * U[0, 0]],\n","         [mu[1], mu[1] + 1.5 * S[0] * U[1, 0]],\n","         color=\"black\", linewidth=4)\n","plt.plot([mu[0], mu[0] + 1.5 * S[1] * U[0, 1]],\n","         [mu[1], mu[1] + 1.5 * S[1] * U[1, 1]],\n","         color=\"black\", linewidth=4)\n","plt.show()"]},{"cell_type":"markdown","id":"afcdf669","metadata":{"id":"afcdf669"},"source":["#### Dimensionality Reduction with PCA"]},{"cell_type":"code","execution_count":null,"id":"afe5ffe8","metadata":{"id":"afe5ffe8"},"outputs":[],"source":["# Function to project original data onto a lower dimensional space defined by the principal components\n","def projectData(X, U, K):\n","    \"\"\"\n","    Projects the data X onto the top K principal components.\n","\n","    Parameters:\n","    X : array_like\n","        The data set, where each row is an example and each column represents a feature.\n","    U : array_like\n","        The matrix of principal components; each column is a principal component.\n","    K : int\n","        The number of principal components to project onto.\n","\n","    Returns:\n","    Z : array_like\n","        The projected data. This output matrix is of shape (number of examples, K).\n","    \"\"\"\n","    # Use numpy dot product to project the data onto the first K columns of U\n","    # X: original data matrix, U[:, :K]: first K principal components (eigenvectors)\n","    Z = np.dot(X, U[:, :K])\n","    return Z"]},{"cell_type":"markdown","id":"0153bfcc","metadata":{"id":"0153bfcc"},"source":["#### Reduce the dimension of dataset to 1"]},{"cell_type":"code","execution_count":null,"id":"f456ebf4","metadata":{"id":"f456ebf4"},"outputs":[],"source":["# Specify the number of principal components to retain\n","K = 1\n","\n","# Project the normalized data X_norm onto the top K (1 in this case) principal components\n","Z = projectData(X_norm, U, K)\n","\n","# Print the first element of the projected data to see the new feature representation\n","print(Z[0])  # Example output: 1.481274"]},{"cell_type":"markdown","id":"ffead004","metadata":{"id":"ffead004"},"source":["#### Reconstruct the dataset to the original dimension"]},{"cell_type":"code","execution_count":null,"id":"57de2215","metadata":{"id":"57de2215"},"outputs":[],"source":["# Function to convert principal components back to the original dimension\n","def recoverData(Z, U, K):\n","    \"\"\"\n","    Reconstructs the original data from its projection onto the top K principal components.\n","\n","    Parameters:\n","    Z : array_like\n","        The projected data, where each row is a data point in the reduced dimensional space.\n","    U : array_like\n","        The matrix of principal components; each column is a principal component.\n","    K : int\n","        The number of principal components that the original data was projected onto.\n","\n","    Returns:\n","    X_rec : array_like\n","        The reconstructed data from the projection. This output has the same number of features as the original dataset.\n","    \"\"\"\n","    # Use numpy dot product to reconstruct the data from its projections\n","    # Z: projected data matrix, U[:, :K].T: transpose of the first K principal components\n","    X_rec = np.dot(Z, U[:, :K].T)\n","    return X_rec\n"]},{"cell_type":"code","execution_count":null,"id":"5ae7c762","metadata":{"id":"5ae7c762"},"outputs":[],"source":["# Reconstruct the dataset from the principal components\n","X_rec = recoverData(Z, U, K)\n","print(X_rec[0])  # Example output showing the first reconstructed data point\n"]},{"cell_type":"markdown","id":"76eca05f","metadata":{"id":"76eca05f"},"source":["#### Visualise both reduced dataset and original dataset"]},{"cell_type":"code","execution_count":null,"id":"d8b1e9aa","metadata":{"id":"d8b1e9aa"},"outputs":[],"source":["# Visualizing the reduced and reconstructed dataset\n","plt.figure(figsize=(8, 4))\n","plt.subplot(121)\n","plt.scatter(X_norm[:, 0], X_norm[:, 1])\n","plt.title(\"Normalized Original Data\")\n","plt.subplot(122)\n","plt.scatter(X_rec[:, 0], X_rec[:, 1])\n","plt.title(\"Reconstructed Data\")\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[{"file_id":"1QkyNNjXCk56DzyrwB7XJumEd1PT4iQ1g","timestamp":1724988495709}]}},"nbformat":4,"nbformat_minor":5}